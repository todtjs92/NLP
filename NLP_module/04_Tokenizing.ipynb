{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 Tokenizing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlXpw5dmnLBj"
      },
      "source": [
        "# 1. 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_DO4C_unHu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c38cb83-0250-46c7-d635-9e8d7e2d786a"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FwwjnvanOa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b730101e-ec91-4946-be2e-e17b90e105b7"
      },
      "source": [
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1CQT4Sear6NKxGiZIW3WpAGkTanO0azrl\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1CQT4Sear6NKxGiZIW3WpAGkTanO0azrl\" -o wiki_20190620_small.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   408    0   408    0     0   2775      0 --:--:-- --:--:-- --:--:--  2775\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 40965  100 40965    0     0   131k      0 --:--:-- --:--:-- --:--:--  131k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sG8RV_WnT8p"
      },
      "source": [
        "#2. Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5Wm-GLgnkaz"
      },
      "source": [
        "input_text = \"수학은 그 구조와 발전 과정에서는 자연과학에 속하는 물리학을 비롯한 다른 학문들과 깊은 연관을 맺고 있다.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W47K1ITknTSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394efd5d-7f1d-4501-ef74-60f2b6efc882"
      },
      "source": [
        "from tokenizers.implementations import SentencePieceBPETokenizer\n",
        "path = '/content/wiki_20190620_small.txt'\n",
        "tokenizer = SentencePieceBPETokenizer()\n",
        "tokenizer.train(files=path, vocab_size=100, min_frequency=2, special_tokens=[\"<unk>\"])\n",
        "tokenizer.save_model(\"/content/\", \"bpe_tokenizer\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/bpe_tokenizer-vocab.json', '/content/bpe_tokenizer-merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy-2003LnfRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d83db0-1aea-4264-dec7-998b947847fb"
      },
      "source": [
        "vocab_file_path = '/content/bpe_tokenizer-vocab.json'\n",
        "merge_file_path = '/content/bpe_tokenizer-merges.txt'\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer(vocab_file_path, merge_file_path)\n",
        "\n",
        "result = tokenizer.encode(input_text).tokens\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', '수', '학', '은', '▁', '그', '▁', '구', '조', '와', '▁', '발', '전', '▁', '과', '정', '에', '서', '는', '▁', '자', '연', '과', '학', '에', '▁', '속', '하', '는', '▁', '물', '리', '학', '을', '▁', '비', '롯', '한', '▁', '다', '른', '▁', '학', '문', '들', '과', '▁', '깊', '은', '▁', '연', '관', '을', '▁', '맺', '고', '▁', '있', '다', '.']\n"
          ]
        }
      ]
    }
  ]
}