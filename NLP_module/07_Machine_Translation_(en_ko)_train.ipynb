{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Translation (en-ko) - train",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w8ew706HC5C"
      },
      "source": [
        "# 기계 번역 모델 학습 \n",
        "\n",
        "이번 실습에서는 기계 번역을 수행하는 RNN 기반 모델의 학습에 대하여 살펴보겠습니다.\n",
        "\n",
        "기계 번역이란 하나의 언어로 적혀져있는 문장을 다른 언어의 문장으로 번역하는 분야를 뜻합니다.\n",
        "\n",
        "룰에 따라 번역을 하는 단순한 기계 번역부터 딥러닝 모델을 이용한 최신 모델까지 다양하게 있습니다.\n",
        "\n",
        "이번 실습에서는 딥러닝 모델 중 그 기초가 되는 RNN을 이용한 Sequence to Sequence 모델을 만들겠습니다.\n",
        "\n",
        "그리고 이를 영어-한국어 병렬 데이터를 이용하여 학습하는 것까지 진행하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjj7XEd3BuDy"
      },
      "source": [
        "## 데이터 파일 다운로드\n",
        "\n",
        "데이터 파일을 다운로드 하기 위해 특수 명령어인 gdown을 사용하였습니다.\n",
        "\n",
        "기계 번역 학습을 위해서는 한 언어로 적혀진 문장과 같은 뜻을 가진 다른 언어로 적혀진 문장이 필요합니다.\n",
        "\n",
        "그렇기에 이러한 데이터를 병렬 데이터라고 부릅니다.\n",
        "\n",
        "예를 들어 아래와 같은 문장들이 데이터입니다.\n",
        "\n",
        "- 영어: Sign here please.\n",
        "\n",
        "- 한국어: 여기에 서명하세요.\n",
        "\n",
        "이러한 데이터를 통해 기계 번역 모델은 학습을 합니다.\n",
        "\n",
        "그리고 테스트 때 새로운 문장을 받으면 그에 해당하는 다른 언어의 문장으로 바꿔야 합니다.\n",
        "\n",
        "- 입력: Do you understand?\n",
        "\n",
        "- 출력: 이해하니?\n",
        "\n",
        "이러한 병렬 데이터는 여러 있지만 이번 실습에서는 영화와 TV 자막으로 구축한 데이터인 `OpenSubtitles`를 사용하겠습니다.\n",
        "\n",
        "데이터 출처: https://opus.nlpl.eu/OpenSubtitles2018.php"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3hwEF6GAiEm"
      },
      "source": [
        "!gdown --id 1DGeE1UY8nAhlYwLIIyNRHJ-SlVaxcURU\n",
        "\n",
        "!gdown --id 1GPUgAlDKn0Wk1TqILJwzqeS84eBiOngU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S03tCoPkG6vr"
      },
      "source": [
        "`train_en_ko.csv` 파일을 열어보면 한 라인에 두 개의 열이 있습니다. \n",
        "\n",
        "첫 번째 열에는 영어 문장이 적혀있고 두 번째 열에는 그 문장과 같은 뜻을 가지는 한국어 문장이 있습니다.\n",
        "\n",
        "이렇게 병렬로 문장이 데이터로 있는 것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xumuS251GFJO"
      },
      "source": [
        "with open(\"train_en_ko.csv\") as csv_f:\n",
        "    head = \"\\n\".join([next(csv_f) for x in range(5)])\n",
        "print(head)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvdTJtt1CcRU"
      },
      "source": [
        "## 라이브러리 로드\n",
        "\n",
        "코드 실행에 필요한 라이브러리를 설치하고 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tmTo5O9aHtH"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgnaF87tCfhj"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import nltk\n",
        "import codecs\n",
        "import csv\n",
        "from konlpy.tag import Okt\n",
        "import torch.nn.functional as F\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlUeGHEJPZeV"
      },
      "source": [
        "\n",
        "## 모델 클래스 정의\n",
        "\n",
        "pytorch는 딥러닝 모델의 forward path를 정의할 때는 반드시 `nn.Module` 클래스로부터 상속을 받아 새로운 클래스로 만들어야 합니다.\n",
        "\n",
        "그리고 그 forward path를 정의하기 위해 반드시 `forward` 함수를 정의하여야 합니다.\n",
        "\n",
        "이번 실습에서는 RNN을 이용한 기계 번역 모델이기에 그에 적합한 클래스를 작성하였습니다.\n",
        "\n",
        "- 문제 1. `GRUMT` 클래스 내 model 구성에 있어 RNN으로 GRU를 사용하겠습니다. `encoder_rnn`과 `decoder_rnn`에 GRU를 정의하세요.\n",
        "  - 힌트 1) GRU의 크기는 `hidden_size`에 맞춰주세요.\n",
        "  - 힌트 2) 간단히 한 방향으로만 움직이는 GRU를 만들어주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pia7XebEPbS1"
      },
      "source": [
        "class GRUMT(nn.Module):\n",
        "    # GRU 기반 MT 클래스를 정의합니다. Pytorch는 모델을 구성할 때 반드시 nn.Module 클래스를 상속받은 후 이를 토대로 만듭니다.\n",
        "    def __init__(self, input_size, hidden_size, output_size, max_length, device, dropout_p=0.1):\n",
        "        # 클래스의 첫 시작인 함수입니다. 여기서 모델에 필요한 여러 변수들을 정의합니다.\n",
        "        super(GRUMT, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.device = device\n",
        "\n",
        "        # Encoder 부분\n",
        "        self.encoder_embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # <ToDo>: encoder를 GRU로 정의하세요.\n",
        "        self.encoder_rnn = None\n",
        "\n",
        "        # Decoder 부분\n",
        "        # <ToDo>: decoder를 GRU로 정의하세요.\n",
        "        self.decoder_rnn = None\n",
        "        self.decoder_embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        self.loss = nn.NLLLoss()\n",
        "\n",
        "    def _encoder(self, input_tensor, input_length):\n",
        "        # forward 함수 중 첫 번째 부분인 encoder에 대한 함수입니다.\n",
        "        encoder_hidden = self._init_hidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(self.max_length, self.hidden_size, device=self.device)\n",
        "\n",
        "        # input_tensor의 길이만큼 하나씩 GRU를 통과시키고 그 결과를 저장합니다.\n",
        "        for idx in range(input_length):\n",
        "            input_tensor_step = input_tensor[idx]\n",
        "            embedded = self.encoder_embedding(input_tensor_step).view(1, 1, -1)\n",
        "            encoder_output, encoder_hidden = self.encoder_rnn(embedded, encoder_hidden)\n",
        "            encoder_outputs[idx] = encoder_output[0, 0]\n",
        "\n",
        "        return encoder_outputs, encoder_hidden\n",
        "\n",
        "    def _decoder(self, target_tensor, target_length, encoder_hidden, encoder_outputs):\n",
        "        # forward 함수 중 두 번째 부분인 decoder에 대한 함수입니다.\n",
        "        # decoder의 입력은 특수 문자인 SOS입니다.\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=self.device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        loss_sum = 0\n",
        "        # 번역할 문장의 길이만큼 단어를 생성합니다. \n",
        "        # 단어 생성은 주어진 언어 사전에 있는 단어 중 하나를 선택하는 classification 문제와 동일합니다.\n",
        "        for di in range(target_length):\n",
        "            embedded = self.decoder_embedding(decoder_input).view(1, 1, -1)\n",
        "            embedded = self.dropout(embedded)\n",
        "\n",
        "            # encoder의 결과와 decoder의 hidden을 결합하여 현재 생성할 단어에 영향을 많이 주는 attention을 구합니다.\n",
        "            decoder_attention = F.softmax(self.attn(torch.cat((embedded[0], decoder_hidden[0]), 1)), dim=1)\n",
        "            attn_applied = torch.bmm(decoder_attention.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "            output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "            output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "            output = F.relu(output)\n",
        "            output, decoder_hidden = self.decoder_rnn(output, decoder_hidden)\n",
        "\n",
        "            decoder_output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "\n",
        "            # decoder를 거쳐 나온 출력 중 가장 높은 값을 가지는 단어를 찾습니다.\n",
        "            # 찾은 단어는 다음 반복문의 입력 단어가 됩니다.\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            target_output = torch.tensor([target_tensor[di]], device=self.device)\n",
        "            \n",
        "            # 그리고 그 단어와 실제 단어의 차이를 loss로 정의합니다.\n",
        "            loss_sum += self.loss(decoder_output, target_output)\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "        return loss_sum\n",
        "\n",
        "    def forward(self, input_tensor, input_length, target_tensor, target_length):\n",
        "        # 모델의 forward feed를 수행하는 함수입니다.\n",
        "        # 영어 문장과 한국어 문장 두 개를 받아 영어 문장에서 한국어 문장을 만드는 seq2seq 모델입니다.\n",
        "        # Encoder 파트\n",
        "        encoder_outputs, encoder_hidden = self._encoder(input_tensor, input_length)\n",
        "\n",
        "        # Decoder 파트\n",
        "        loss_sum = self._decoder(target_tensor, target_length, encoder_hidden, encoder_outputs)\n",
        "\n",
        "        return loss_sum\n",
        "\n",
        "    def _init_hidden(self):\n",
        "        # encoder와 decoder 둘 다 처음에 가지는 hidden입니다. 간단히 0으로 시작합니다.\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=self.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0C0fY54ZDmU"
      },
      "source": [
        "## 데이터 클래스 정의\n",
        "\n",
        "이번 실습에서는 데이터를 불러올 때 토큰을 만들 수 있도록 데이터 클래스와 이를 만드는 함수 `make_data_loader`를 정의하겠습니다.\n",
        "\n",
        "이에 앞서 언어별 단어 사전을 만드는 클래스를 정의하여 사용하겠습니다.\n",
        "\n",
        "이전 실습에서는 `Field` 내 `build_vocab` 함수를 통해 단어 사전을 만들었습니다. 이를 이번에는 직접 구현해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgRv_KuZpEzA"
      },
      "source": [
        "단어 사전을 만들 때 있어 기본적으로 가지는 특수한 단어는 3개가 있습니다.\n",
        "\n",
        "- SOS: Start of the sentence, 문장의 시작을 알리는 단어\n",
        "- EOS: End of the sentence, 문장의 끝을 알리는 단어\n",
        "- UNK: Unknown word, 모르는 단어\n",
        "\n",
        "SOS와 EOS는 각각 문장의 처음과 끝을 알리는 단어입니다.\n",
        "\n",
        "decoder는 SOS를 처음 입력으로 받아 그로부터 문장 생성을 시작합니다.\n",
        "\n",
        "그리고 EOS를 생성하게 되면 단어 생성을 멈추는 것입니다.\n",
        "\n",
        "UNK는 모르는 단어로 out of vocabulary를 표현할 때 사용합니다.\n",
        "\n",
        "즉, training 데이터에는 나타나지 않지만 validation 혹은 test 데이터에 나타난 단어가 있을 때 그 단어에 대한 표시를 하는 것입니다.\n",
        "\n",
        "이러한 단어를 무시하는 것도 하나의 방법입니다. 하지만 그런 경우 모르는 단어가 존재한다는 정보가 사라지기에 이를 보완하고자 특수 단어로 표시하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH-Goaky8J5w"
      },
      "source": [
        "class LangDic:\n",
        "    # 언어마다 단어 사전을 정의합니다.\n",
        "    def __init__(self, name, tokenizer):\n",
        "        # 클래스의 첫 시작인 함수입니다. 여기서 모델에 필요한 여러 변수들을 정의합니다.\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n",
        "        self.n_words = 2\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        # 문장을 받아 문장에서 단어를 확인합니다.\n",
        "        for word in self.tokenizer(sentence):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # 단어를 보고 그 단어가 사전에 존재하는지 아닌지를 살펴봅니다.\n",
        "        # 존재하지 않는 경우 단어를 사전에 등록합니다.\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def sentence2tensor(self, sentence, max_len):\n",
        "        # 가지고 있는 사전을 바탕으로 문장을 tensor의 형태로 바꿉니다.\n",
        "        # tensor 내 들어있는 값은 단어 index이며 이를 통해 모델의 임베딩에 입력으로 줄 수 있습니다.\n",
        "        indexes = list()\n",
        "        for word in self.tokenizer(sentence):\n",
        "            try:\n",
        "                indexes.append(self.word2index[word])\n",
        "            except KeyError:\n",
        "                indexes.append(UNK_token)\n",
        "\n",
        "        indexes.append(EOS_token)\n",
        "        len_sen = len(indexes)\n",
        "        if len_sen > max_len:\n",
        "            indexes = indexes[:max_len-1]\n",
        "            indexes.append(EOS_token)\n",
        "            len_sen = max_len\n",
        "\n",
        "        index_tensor = torch.tensor(indexes)\n",
        "        return index_tensor, len_sen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoyLsQU08N2D"
      },
      "source": [
        "def make_dic(dataset_path):\n",
        "    # 사전을 만드는 함수입니다.\n",
        "    data_pairs = load_file(dataset_path)\n",
        "\n",
        "    # 영어의 경우 NLTK tokenizer를\n",
        "    # 한국어의 경우 Konlpy 내 Open Korean Text tokenizer를 이용합니다.\n",
        "    eng_tokenizer = word_tokenize\n",
        "    kor_tokenizer = Okt().morphs\n",
        "\n",
        "    eng_dic = LangDic('en', eng_tokenizer)\n",
        "    kor_dic = LangDic('ko', kor_tokenizer)\n",
        "\n",
        "    for eng_sen, kor_sen in data_pairs:\n",
        "        eng_dic.add_sentence(eng_sen)\n",
        "        kor_dic.add_sentence(kor_sen)\n",
        "\n",
        "    return eng_dic, kor_dic\n",
        "\n",
        "def load_file(dataset_path):\n",
        "    # 데이터를 읽는 함수입니다.\n",
        "    data_pairs = list()\n",
        "    # 데이터 파일의 내용을 불러와 영어 문장과 한국어 문장을 모아 리스트에 넣습니다.\n",
        "    with codecs.open(dataset_path, \"r\", \"utf-8\") as csv_f:\n",
        "        csv_reader = csv.reader(csv_f)\n",
        "        for one_row in csv_reader:\n",
        "            data_pairs.append(one_row)\n",
        "\n",
        "    return data_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEF1pQoYZRqA"
      },
      "source": [
        "class MTDataset(Dataset):\n",
        "    # pytorch로 데이터를 불러오기 위해서 Dataset 클래스를 상속받아 새로운 클래스를 만듭니다.\n",
        "    def __init__(self, data_pairs, eng_dic, kor_dic, max_len):\n",
        "        super(MTDataset, self).__init__()\n",
        "\n",
        "        # 데이터를 파일로부터 읽어 이를 전달 받습니다.\n",
        "        self.max_len = max_len\n",
        "        self.pair_data = list()\n",
        "\n",
        "        # 데이터 내 문장을 미리 정의한 사전에 기반하여 tensor로 바꿉니다.\n",
        "        for eng_sen, kor_sen in data_pairs:\n",
        "            eng_sen_words, eng_sen_len = eng_dic.sentence2tensor(eng_sen, max_len)\n",
        "            kor_sen_words, kor_sen_len = kor_dic.sentence2tensor(kor_sen, max_len)\n",
        "            self.pair_data.append((eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len))\n",
        "\n",
        "        self.data_len = len(self.pair_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx번째 데이터를 반환합니다.\n",
        "        eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len = self.pair_data[idx]\n",
        "\n",
        "        return eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nATcgR-UZV-I"
      },
      "source": [
        "def make_data_loader(dataset_path, eng_dic, kor_dic, max_len, batch_size):\n",
        "    # DataLoader를 만들어서 데이터를 불러오도록 합니다.\n",
        "    data_pairs = load_file(dataset_path)\n",
        "\n",
        "    # 앞서 정의한 MTDataset 클래스에 해당 데이터를 넣습니다.\n",
        "    ds = MTDataset(data_pairs, eng_dic, kor_dic, max_len)\n",
        "\n",
        "    # 만들어진 MTDataset 클래스를 DataLoader에 넣고 batch 크기를 전달해줍니다.\n",
        "    return DataLoader(ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZhhZRkJChKT"
      },
      "source": [
        "## train 함수\n",
        "\n",
        "해당 함수에서는 정의된 `model` 클래스의 인스턴스를 가져와서 이를 train data로 학습시킵니다. 그리고 validation data로 학습 중간에 성능을 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYucrg5wCjyA"
      },
      "source": [
        "def train(model, device, optimizer, train_loader, valid_loader, num_epochs):\n",
        "    # 학습에 필요한 변수들을 기본적으로 정의합니다.\n",
        "    running_loss = 0.0\n",
        "    global_step = 0\n",
        "    eval_every = 100\n",
        "\n",
        "    # model에게 학습이 진행됨을 알려줍니다.\n",
        "    model.train()\n",
        "    # num_epochs만큼 epoch을 반복합니다.\n",
        "    for epoch in range(num_epochs):\n",
        "        # train_loader를 읽으면 정해진 데이터를 읽어옵니다.\n",
        "        for input_tensor, input_length, target_tensor, target_length in train_loader:\n",
        "            # 데이터를 GPU로 옮깁니다.\n",
        "            input_tensor = input_tensor[0].to(device)\n",
        "            target_tensor = target_tensor[0].to(device)\n",
        "\n",
        "            # model을 함수처럼 호출하면 model에서 정의한 forward 함수가 실행됩니다.\n",
        "            # 즉, 데이터를 모델에 집어넣어 forward방향으로 흐른 후 그 결과를 받습니다.\n",
        "            loss_sum = model(input_tensor, input_length, target_tensor, target_length)\n",
        "\n",
        "            # 최적화 수행\n",
        "            optimizer.zero_grad()\n",
        "            loss_sum.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss_sum.item() / target_length.item()\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % eval_every == 0:\n",
        "                # 100번에 한 번으로 validation 데이터를 이용하여 성능을 검증합니다.\n",
        "                print_loss_avg = running_loss / eval_every\n",
        "\n",
        "                average_valid_loss = evaluate(model, device, valid_loader)\n",
        "\n",
        "                # 검증이 끝난 후 다시 모델에게 학습을 준비시킵니다.\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "\n",
        "                # 결과 출력\n",
        "                print('Epoch {}, Step {}, Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch + 1, global_step, print_loss_avg, average_valid_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz8Q3e9iQroL"
      },
      "source": [
        "## evaluate 함수\n",
        "\n",
        "해당 함수에서는 validation data를 이용하여 학습된 `model`을 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PcidZwmQuFu"
      },
      "source": [
        "def evaluate(model, device, valid_loader):\n",
        "    # 학습 중 모델을 평가합니다.\n",
        "    # 모델에게 학습이 아닌 평가를 할 것이라고 알립니다.\n",
        "    model.eval()\n",
        "    valid_running_loss = 0.0\n",
        "\n",
        "    # 학습이 아니기에 최적화를 하지 않는다는 환경을 설정합니다.\n",
        "    with torch.no_grad():\n",
        "        # validation 데이터를 읽습니다.\n",
        "        for input_tensor, input_length, target_tensor, target_length in valid_loader:\n",
        "            input_tensor = input_tensor[0].to(device)\n",
        "            target_tensor = target_tensor[0].to(device)\n",
        "\n",
        "            # model을 함수처럼 호출하면 model에서 정의한 forward 함수가 실행됩니다.\n",
        "            # 즉, 데이터를 모델에 집어넣어 forward방향으로 흐른 후 그 결과를 받습니다.\n",
        "            loss_sum = model(input_tensor, input_length, target_tensor, target_length)\n",
        "\n",
        "            # validation 데이터의 loss, 즉 모델의 출력과 실제 데이터의 차이를 구합니다.\n",
        "            valid_running_loss += loss_sum.item() / target_length.item()\n",
        "\n",
        "    # 평균 loss를 계산하여 반환합니다.\n",
        "    return valid_running_loss / len(valid_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra8NgFTOCy-p"
      },
      "source": [
        "## 데이터 불러오기\n",
        "\n",
        "데이터를 불러올 때 우리는 먼저 언어별 사전을 정의하고 그 사전에 맞춰 데이터를 불러옵니다.\n",
        "\n",
        "- 문제 2. `valid_loader`를 불러오세요. 힌트) `train_loader`을 참고하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jy8JWuPCyvC"
      },
      "source": [
        "on_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 사전 내 특수 단어의 index를 각각 미리 정의합니다.\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "# 데이터의 파일 정보\n",
        "train_file_path = \"./train_en_ko.csv\"\n",
        "valid_file_path = \"./valid_en_ko.csv\"\n",
        "\n",
        "max_len = 10\n",
        "hidden_size = 256\n",
        "\n",
        "# 언어 별 사전 생성\n",
        "eng_dic, kor_dic = make_dic(dataset_path=train_file_path)\n",
        "\n",
        "#  train, validation 데이터 csv 파일을 읽어옵니다.\n",
        "train_loader = make_data_loader(train_file_path, eng_dic, kor_dic, max_len, 1)\n",
        "\n",
        "# <ToDo>: valid_dataset을 불러오세요.\n",
        "valid_loader = None\n",
        "\n",
        "# 영어와 한국어 사전의 크기(단어 개수)를 가져옵니다.\n",
        "eng_dic_size = eng_dic.n_words\n",
        "kor_dic_size = kor_dic.n_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNlcKZKoDDEq"
      },
      "source": [
        "## 모델 학습\n",
        "\n",
        "- 문제 3. `GRUMT` 인스턴스를 만드세요.\n",
        "- 문제 4. `train` 함수를 이용하여 train data를 통해 모델 학습을 진행하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpjT1fyTDF15"
      },
      "source": [
        "# <ToDo>: GRUMT 클래스의 인스턴스를 만드세요. 인스턴스 생성 시 필요한 parameter를 전달해주세요.\n",
        "model = GRUMT(None).to(on_device)\n",
        "\n",
        "# Adam optimizier를 사용합니다.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# <ToDo>: 학습을 위해 train 함수의 적절한 parameter를 전달해주세요.\n",
        "train(None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}