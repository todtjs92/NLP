{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Seq2SeqWithAttention.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lWPhKRrqw8Vl"},"source":["### 필요 라이브러리 임포트"]},{"cell_type":"code","metadata":{"id":"TF6swkROGoRk","executionInfo":{"status":"ok","timestamp":1660647774680,"user_tz":-540,"elapsed":1972,"user":{"displayName":"김정의","userId":"05657577236532280758"}}},"source":["import os   # directory 생성 및 디렉토리 생성과 관련된 package \n","import json \n","import random\n","from random import choice, randrange # random\n","import numpy as np  # numpy \n","import editdistance # 평가 지표로서 사용될 edit distance \n","import matplotlib.pyplot as plt # 그래프를 그리기 위한 라이브러리\n","import tqdm\n","import torch # torch library\n","import torch.nn as nn # Nueral Network에 대한 package\n","import torch.nn.functional as F # pytorch function 들을 사용하기 위한 용도 \n","from torch.utils import data # dataset 관련된 utility 를 사용하려는 용도"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# PyTorch 학습\n","PyTorch (torch) 라이브러리를 이용해 학습을 진행하기 위해서는 크게 3가지 요소를 구현해야합니다.\n","- 데이터\n","- 모델\n","- 학습방법\n","\n","### 데이터\n","- 데이터셋은 딥러닝 모델의 방향성을 정하는 요소입니다.\n","  - 동일한 모델을 이용해 번역, 요약, QA 등 다양한 태스크를 학습할 수 있고, 이를 결정하는 요소가 데이터셋입니다.\n","- PyTorch에서는 torch.utils.data 클래스를 이용해 Dataset(1개의 데이터 처리), Dataloader(여러개의 데이터 병합)를 구현합니다.\n","- 오늘 실습에서는 입력된 알파벳을 거꾸로 출력하는 간단한 토이데이터를 사용할 예정입니다.\n","  - ex) abcde -> edcba, aacca -> accaa \n","\n","### 모델\n","- PyTorch에서는 torch.nn 클래스를 사용하여 모델을 구현합니다.\n","- 오늘 실습할 내용은 RNN with Bahdanau attention 모델입니다.\n","\n","### 학습방법\n","- loss function\n","- optimizer\n","- hyperparameters\n","  - batch size\n","  - learning rate\n","  - training epochs\n","- (optional)\n","  - weight decay\n","  - early stopping\n","  - learning rate scheduler"],"metadata":{"id":"EEa-EgIPj8jQ"}},{"cell_type":"markdown","metadata":{"id":"dvgO0Z-3xKpr"},"source":["### Data Loader "]},{"cell_type":"code","metadata":{"id":"QAaKIfA7BdLX","executionInfo":{"status":"ok","timestamp":1660647816507,"user_tz":-540,"elapsed":307,"user":{"displayName":"김정의","userId":"05657577236532280758"}}},"source":["class ToyDataset(data.Dataset):\n","    def __init__(self, min_length=5, max_length=20, type='train'):\n","        self.SOS = \"<s>\"  # all strings will end with the End Of String token )\n","        self.EOS = \"</s>\"  # all strings will end with the End Of String token\n","        self.characters = list(\"abcdefg\")\n","        self.int2char = list(self.characters)\n","        self.char2int = {c: i+3 for i, c in enumerate(self.characters)} # +3 을 왜하는 가?\n","        print(self.char2int)\n","        self.VOCAB_SIZE = len(self.characters)\n","        self.min_length = min_length\n","        self.max_length = max_length\n","        \n","        # train set or test set 을 생성\n","        if type == 'train':\n","            self.set = [self._sample() for _ in range(4000)]\n","        else:\n","            self.set = [self._sample() for _ in range(300)]\n","\n","    def __len__(self):\n","        return len(self.set)\n","\n","    def __getitem__(self, item):\n","        return self.set[item]\n","\n","    def _sample(self):\n","        random_length = randrange(self.min_length, self.max_length)  # Pick a random length\n","        random_char_list = [choice(self.characters[:-1]) for _ in range(random_length)]  # Pick random chars\n","        random_string = ''.join(random_char_list)\n","        a = np.array([self.char2int.get(x) for x in random_string]+[2])\n","        b = np.array([self.char2int.get(x) for x in random_string[::-1]] + [2]) # Return the random string and its reverse + EOS \n","        \n","        return a, b\n","\n","\n","def pad_tensor(vec, pad, value=0, dim=0):\n","    \"\"\"\n","    pad token으로 채우는 용도 \n","    args:\n","        vec - tensor to pad\n","        pad - the size to pad to\n","        dim - dimension to pad\n","    return:\n","        a new tensor padded to 'pad' in dimension 'dim'\n","    \"\"\"\n","    pad_size = pad - vec.shape[0]\n","\n","    if len(vec.shape) == 2:\n","        zeros = torch.ones((pad_size, vec.shape[-1])) * value\n","    elif len(vec.shape) == 1:\n","        zeros = torch.ones((pad_size,)) * value\n","    else:\n","        raise NotImplementedError\n","    return torch.cat([torch.Tensor(vec), zeros], dim=dim)\n","\n","\n","\n","def pad_collate(batch, values=(0, 0), dim=0):\n","    \"\"\"\n","    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n","    args:\n","        batch - list of (tensor, label)\n","    reutrn:\n","        xs - a tensor of all examples in 'batch' after padding\n","        ys - a LongTensor of all labels in batch\n","        ws - a tensor of sequence lengths\n","    \"\"\"\n","\n","    sequence_lengths = torch.Tensor([int(x[0].shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n","    sequence_lengths, xids = sequence_lengths.sort(descending=True) # 감소하는 순서로 정렬\n","    target_lengths = torch.Tensor([int(x[1].shape[dim]) for x in batch])\n","    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n","    src_max_len = max(map(lambda x: x[0].shape[dim], batch))\n","    tgt_max_len = max(map(lambda x: x[1].shape[dim], batch))\n","    # pad according to max_len (max length 만큼 padd를 추가 )\n","    batch = [(pad_tensor(x, pad=src_max_len, dim=dim), pad_tensor(y, pad=tgt_max_len, dim=dim)) for (x, y) in batch]\n","\n","    # stack all\n","    xs = torch.stack([x[0] for x in batch], dim=0)\n","    ys = torch.stack([x[1] for x in batch], dim=0)\n","    xs = xs[xids].contiguous() # decreasing order로 다시 나열 \n","    ys = ys[xids].contiguous() # xids 와 같은 순서로 \n","    target_lengths = target_lengths[xids] \n","    return xs.long(), ys.long(), sequence_lengths.int(), target_lengths.int()\n","\n","\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kXtIVGUgHmws"},"source":["## Encoder RNN"]},{"cell_type":"markdown","metadata":{"id":"LdFDvktffm9k"},"source":["![대체 텍스트](https://pytorch.org/tutorials/_images/encoder-network.png)"]},{"cell_type":"markdown","metadata":{"id":"Qf62OoGcwJP5"},"source":["#### Embedding Module "]},{"cell_type":"markdown","metadata":{"id":"c8mNg3ve8LmT"},"source":["![대체 텍스트](https://i.ibb.co/S0NJzq7/embedding.png)"]},{"cell_type":"markdown","metadata":{"id":"kH2OnZrXwOqk"},"source":["#### GRU Module"]},{"cell_type":"markdown","metadata":{"id":"goSiSeiq8bn-"},"source":["![대체 텍스트](https://i.ibb.co/881BygH/GRU.png)"]},{"cell_type":"markdown","metadata":{"id":"L_TIF6h58dZf"},"source":["![대체 텍스트](https://i.ibb.co/NsMqvcH/GRU-param.png)"]},{"cell_type":"markdown","metadata":{"id":"g-Z_vgd2wSAd"},"source":["#### ENCODER RNN Code"]},{"cell_type":"markdown","metadata":{"id":"7t-W0gQps2rg"},"source":["![대체 텍스트](https://i.stack.imgur.com/Kuhh0.jpg)"]},{"cell_type":"code","metadata":{"id":"8d__61m2HfFR","executionInfo":{"status":"ok","timestamp":1660648028995,"user_tz":-540,"elapsed":295,"user":{"displayName":"김정의","userId":"05657577236532280758"}}},"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","class EncoderRNN(nn.Module):\n","    def __init__(self, config):\n","        super(EncoderRNN, self).__init__()\n","        self.input_size = config[\"n_channels\"]\n","        self.hidden_size = config[\"encoder_hidden\"]\n","        self.layers = config.get(\"encoder_layers\", 1)\n","        \n","        self.dropout = config.get(\"encoder_dropout\", 0.) \n","        self.bi = config.get(\"bidirectional_encoder\", False)\n","        embedding_dim = config.get(\"embedding_dim\", None)\n","        self.embedding_dim = embedding_dim if embedding_dim is not None else self.hidden_size\n","        self.embedding = nn.Embedding(config.get(\"n_classes\", 32), self.embedding_dim, padding_idx=0)\n","        gru_input_dim = self.embedding_dim\n","        self.rnn = nn.GRU(\n","            gru_input_dim,\n","            self.hidden_size, # hidden state = 아웃풋으로 나오는 벡터의 사이즈\n","            self.layers,      # 시간 부분은 없구나 그래서 이랫군\n","            dropout=self.dropout,\n","            bidirectional=self.bi,\n","            batch_first=True)# model 선언 \n","\n","            # batch_size, 를 앞으로 두겠느냐는 뜼이라네 ? \n","\n","        self.gpu = config.get(\"gpu\", False) \n","\n","\n","\n","    def forward(self, inputs, hidden, input_lengths):\n","        \n","        # pack padded 를 통하여 input을 감싸기 \n","        inputs = self.embedding(inputs)\n","        # [batch_size, sequence_length,  embedding_size]\n","        # [[1,2,3],[2,3,4],[3,4,5]] 배치는 1 , 시퀀스 길이별, 임베딩\n","        \n","        x = pack_padded_sequence(inputs, input_lengths, batch_first=True) # pad 빼고 학습시키게 해주는 게 있다고하네 \n","        output, state = self.rnn(x, hidden)\n","        # batch , sequence_length ,hidden_dimmension *2 , bidire 이면 2개나옴. \n","        \n","        output, _ = pad_packed_sequence(output, batch_first=True, padding_value=0.) # sequence 를 위의 그림과 같이 pack함 \n","        \n","        if self.bi: # bidirectional 의 경우 forward와 backward를 sum하여 사용한다. or concat \n","            output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]\n","            state = state[:1] + state[1:]\n","        return output, state\n","\n","    def init_hidden(self, batch_size):\n","        # hidden state가 없는 초기 상태일때 \n","        h0 = torch.zeros(2 if self.bi else 1, batch_size, self.hidden_size)\n","        if self.gpu:\n","            h0 = h0.cuda()\n","        return h0"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8yDzSXKHnyB"},"source":["### Decoder "]},{"cell_type":"code","metadata":{"id":"X83Lj_GBHojW","executionInfo":{"status":"ok","timestamp":1660648068745,"user_tz":-540,"elapsed":304,"user":{"displayName":"김정의","userId":"05657577236532280758"}}},"source":["class Decoder(nn.Module):\n","    def __init__(self, config):\n","        super(Decoder, self).__init__()\n","        self.batch_size = config[\"batch_size\"]\n","        self.hidden_size = config[\"decoder_hidden\"]\n","        embedding_dim = config.get(\"embedding_dim\", None)\n","        self.embedding_dim = embedding_dim if embedding_dim is not None else self.hidden_size\n","        self.embedding = nn.Embedding(config.get(\"n_classes\", 32), self.embedding_dim, padding_idx=0)\n","        self.rnn = nn.GRU(\n","            input_size=self.embedding_dim+self.hidden_size if config['decoder'].lower() == 'bahdanau' else self.embedding_dim,\n","            hidden_size=self.hidden_size,\n","            num_layers=config.get(\"decoder_layers\", 1),\n","            dropout=config.get(\"decoder_dropout\", 0),\n","            bidirectional=False,\n","            batch_first=True)\n","        if config['decoder'] != \"RNN\":\n","            self.attention = Attention(\n","                self.batch_size,\n","                self.hidden_size,\n","                method=config.get(\"attention_score\", \"dot\"))\n","\n","        self.gpu = config.get(\"gpu\", False)\n","        self.decoder_output_fn = F.log_softmax if config.get('loss', 'NLL') == 'NLL' else None\n","\n","    def forward(self, **kwargs):\n","        \"\"\" Must be overrided \"\"\"\n","        raise NotImplementedError"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBkw-82hE3MU"},"source":["![대체 텍스트](https://drive.google.com/uc?id=1KNJWaXuEWHsQThRfJUtKYGE_UyE4-cgw)"]},{"cell_type":"markdown","metadata":{"id":"jcUzMxqV9Lr2"},"source":["![대체 텍스트](https://i.ibb.co/gvpn1RT/bmm.png)"]},{"cell_type":"code","metadata":{"id":"IYbzCRP6HuLD"},"source":["class BahdanauDecoder(Decoder):\n","    \"\"\"\n","        Corresponds to BahdanauAttnDecoderRNN in Pytorch tutorial\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super(BahdanauDecoder, self).__init__(config)\n","        self.output_size = config.get(\"n_classes\", 32)\n","        self.character_distribution = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, **kwargs):\n","        \"\"\"\n","        :param input: [B] # 한단어에 대한 설명임 . 시퀀스중에 하나 \n","        :param prev_context: [B, H]\n","        :param prev_hidden: [B, H]\n","        :param encoder_outputs: [B, T, H]\n","        :return: output (B), context (B, H), prev_hidden (B, H), weights (B, T)\n","        \"\"\"\n"," \n","        input = kwargs[\"input\"] # decoder input # 학습할떄는 디코더에 한번에 다 넣어서 학습시켜줌 . \n","        prev_hidden = kwargs[\"prev_hidden\"] # decoder rnn 에서 들어갈 previous hidden state  # 인코더에서 나온 맨마지막꺼 히든 벡터 \n","        encoder_outputs = kwargs[\"encoder_outputs\"] # encoder RNN에서 Encoding이 끝난 (B,L,hidden_size)   # 모든거 합친거 , 어텐션에서만 있는 개념 . \n","        seq_len = kwargs.get(\"seq_len\", None) # sequence length \n","\n","        # check inputs\n","        \n","       \n","\n","        # Attention weights\n","        weights = self.attention.forward(prev_hidden, encoder_outputs, seq_len)  # B x T\n","        context = weights.unsqueeze(1).bmm(encoder_outputs).squeeze(1)  # [B x H]\n","\n","        # embed characters\n","        embedded = self.embedding(input).unsqueeze(0) #  [1,b,h]\n","        \n","        #attention 을 통해 얻어낸 context를 추가하여 모델에 input으로 제공\n","        rnn_input = torch.cat((embedded, context.unsqueeze(0)), 2)\n","\n","        # b,1, 2h 가 나옴?? transpose결과 \n","\n","        outputs, hidden = self.rnn(rnn_input.transpose(1, 0), prev_hidden.unsqueeze(0)) # B x 1 x H, 1 x B x H\n","\n","        \n","        output = self.character_distribution(outputs.squeeze(0)) # logit 값 각 chracter 별로 , 이거 스퀴즈 안해도됨 . \n","\n","        if self.decoder_output_fn:\n","            # NLL loss 인 경우 \n","            output = self.decoder_output_fn(output, -1)\n","\n","        if len(output.size()) == 3:\n","            output = output.squeeze(1)\n","\n","        return output, hidden.squeeze(0), weights\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eHhtZXhUEQ64"},"source":["![대체 텍스트](https://i.stack.imgur.com/tiQkz.png)"]},{"cell_type":"code","metadata":{"id":"kYfGqtu0eqZy"},"source":["def mask_3d(inputs, seq_len, mask_value=0.):\n","    batches = inputs.size()[0]\n","    assert batches == len(seq_len) # length 체크 \n","    max_idx = max(seq_len) # max length 체크 \n","    for n, idx in enumerate(seq_len): # length 에서 의미없는 hidden state attention 값은 0으로 두기 위한 mask값 설정 \n","        if idx < max_idx.item():\n","            if len(inputs.size()) == 3:\n","                inputs[n, idx.int():, :] = mask_value\n","            else:\n","                assert len(inputs.size()) == 2, \"The size of inputs must be 2 or 3, received {}\".format(inputs.size())\n","                inputs[n, idx.int():] = mask_value\n","    return inputs\n","\n","class Attention(nn.Module):\n","    \"\"\"\n","    Inputs:\n","        last_hidden: (batch_size, hidden_size)\n","        encoder_outputs: (batch_size, max_time, hidden_size)\n","    Returns:\n","        attention_weights: (batch_size, max_time)\n","    \"\"\"\n","    def __init__(self, batch_size, hidden_size, method=\"dot\"):\n","        super(Attention, self).__init__()\n","        self.method = method\n","        self.hidden_size = hidden_size\n","        if method == 'dot':\n","            pass\n","        elif method == 'general':\n","            # Wa (hidden,hidden)\n","            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n","        elif method == \"concat\":\n","            # Wa : (2*hidden,hidden)\n","            # Va : (hidden,1)\n","            self.Wa = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","            self.va = nn.Parameter(torch.FloatTensor(hidden_size, 1))\n","        elif method == 'bahdanau':\n","            # Wa : (hidden_size,hidden_size) \n","            # Ua : (hidden_size,hidden_size)\n","            # Va : (hidden_size,1)\n","            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n","            self.Ua = nn.Linear(hidden_size, hidden_size, bias=False)\n","            self.va = nn.Parameter(torch.FloatTensor(hidden_size, 1))\n","        else:\n","            raise NotImplementedError\n","\n","        \n","    def forward(self, last_hidden, encoder_outputs, seq_len=None):\n","        \"\"\"\n","        Inputs :\n","          last_hidden : (B,T,hidden_size)\n","          encoder_outputs : \n","          seq_len:  \n","        Returns:\n","          attention matrix : \n","        \"\"\"\n","        batch_size, seq_lens, _ = encoder_outputs.size()\n","        # attention energies 를 구하기 \n","        attention_energies = self._score(last_hidden, encoder_outputs, self.method)\n","        \n","        if seq_len is not None:\n","            attention_energies = mask_3d(attention_energies, seq_len, -float('inf'))\n","\n","        return F.softmax(attention_energies, -1)\n","\n","    def _score(self, last_hidden, encoder_outputs, method):\n","        \"\"\"\n","        Computes an attention score\n","        :param last_hidden: (batch_size, hidden_dim)\n","        :param encoder_outputs: (batch_size, max_time, hidden_dim)\n","        :param method: str (`dot`, `general`, `concat`)\n","        :return:\n","        \"\"\"\n","\n","        # assert last_hidden.size() == torch.Size([batch_size, self.hidden_size]), last_hidden.size()\n","        \n","        if method == 'dot':\n","            last_hidden = last_hidden.unsqueeze(-1) # (batch_size, hidden_dim,1)\n","            \n","            # attention : (batch_size,max_time, hidden_dim) , (batch_size,hidden_dim,1) - > (batch_size,max_time ,1)\n","            \n","            return encoder_outputs.bmm(last_hidden).squeeze(-1)  \n","\n","        elif method == 'general':\n","            # dot 이랑 비슷, 다만 last hidden을 한번 projection\n","            x = self.Wa(last_hidden) # (batch_size, hidden_dim) ->  (batch_size, hidden_dim)\n","            x = x.unsqueeze(-1) # (batch_size, hidden_dim) ->  (batch_size, hidden_dim,1)\n","            # encoded 된 hidden states 와 dot proudct를 수행하기 \n","            # attention: (batch_size,max_time, hidden_dim) , (batch_size,hidden_dim,1) - > (batch_size,max_time ,1)\n","            return encoder_outputs.bmm(x).squeeze(-1)\n","\n","        elif method == \"concat\":\n","            x = last_hidden.unsqueeze(1).expand_as(encoder_outputs) # (batch_size, hidden_dim) ->  (batch_size,1, hidden_dim)\n","            # concat 후 -> linear 거치기 -> 후 tanh\n","            x = torch.tanh(self.Wa(torch.cat((x, encoder_outputs), -1))) # (batch_size, max_timestep, hidden_dim) ->  (batch_size,  max_timestep, hidden_dim*2)\n","            # (batch_size, max_timestep, hidden_dim*2) ->  (batch_size,  max_timestep, )\n","            return x.matmul(self.va).squeeze(-1)\n","\n","        elif method == \"bahdanau\":\n","            # mlp 기반의 attention model\n","            \n","            x = last_hidden.unsqueeze(1) # (batch_size, hidden_dim) ->  (batch_size,1, hidden_dim)\n","            # 각각을 projection 후 더하기 -> tanh \n","            out = torch.tanh(self.Wa(x) + self.Ua(encoder_outputs)) # \n","            return out.matmul(self.va).squeeze(-1)# (batch_size,max_timestep,hidden_dim) ->  (batch_size, max_timestep)\n","\n","        else:\n","            raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JqeFcsDJ7Dz2"},"source":["### Seq2Seq Model "]},{"cell_type":"markdown","metadata":{"id":"jS2cmY548vtB"},"source":["![대체 텍스트](https://i.ibb.co/CK5wTz5/crossentropy.png)"]},{"cell_type":"markdown","metadata":{"id":"z_rIN2DY80pD"},"source":["![대체 텍스트](https://i.ibb.co/ssXZ28q/crossentropy-2.png)"]},{"cell_type":"code","metadata":{"id":"V-axgHRo7C5V"},"source":["class Seq2Seq(nn.Module):\n","    \"\"\"\n","        Sequence to sequence module\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super(Seq2Seq, self).__init__()\n","        self.SOS = config.get(\"start_index\", 1) # Start index를 가져옵니다. \n","        self.vocab_size = config.get(\"n_classes\", 32) # embedding 에 필요한 vocabulary size \n","        self.batch_size = config.get(\"batch_size\", 1) # batch_size 정보를 가져옵니다.\n","        self.gpu = config.get(\"gpu\", False) # cuda 로 돌아가는지 아닌지에 대한 정보 \n","\n","        # Encoder 선언\n","        \n","        self.encoder = EncoderRNN(config)\n","\n","        # Decoder 선언 \n","        \n","        self.decoder = BahdanauDecoder(config)\n","        \n","        # loss fucntion   \n","        # ignore_index=0 은 왜 입력하는 걸까요? # padding에는 로스가 안가게\n","        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n","        \n","        \n","\n","    def encode(self, x, x_len):\n","        # encoder를 통해 주어진 source 정보를 Encodeing 하는 용도 \n","        \n","        batch_size = x.size()[0]\n","        # 초기 inital hidden state 만들기\n","        init_state = self.encoder.init_hidden(batch_size)\n","        # encoder Forward 수행 \n","        encoder_outputs, encoder_state = self.encoder.forward(x, init_state, x_len)\n","        \n","        \n","       \n","        return encoder_outputs, encoder_state.squeeze(0)\n","\n","    def decode(self, encoder_outputs, encoder_hidden, targets, targets_lengths, input_lengths):\n","        \"\"\"\n","        Args:\n","            encoder_outputs: (B, T, H)\n","            encoder_hidden: (B, H)\n","            targets: (B, L)\n","            targets_lengths: (B)\n","            input_lengths: (B)\n","        Vars:\n","            decoder_input: (B)\n","            decoder_context: (B, H)\n","            hidden_state: (B, H)\n","            attention_weights: (B, T)\n","        Outputs:\n","            alignments: (L, T, B)\n","            logits: (B*L, V)\n","            labels: (B*L)\n","        \"\"\"\n","\n","        batch_size = encoder_outputs.size()[0]\n","        max_length = targets.size()[1]\n","        # decoder의 처음 y0 는 무엇이 되어야 할까? *주의해야할 포인트 \n","        if batch_size ==1:\n","          decoder_input = torch.LongTensor([self.SOS] * batch_size)\n","        else:\n","          decoder_input = torch.LongTensor([self.SOS] * batch_size).squeeze(-1)\n","        decoder_context = encoder_outputs.transpose(1, 0)[-1] #(Batch,1)\n","        decoder_hidden = encoder_hidden\n","        \n","        #alignments :  attention align을 저장하기 위한 용도  \n","        alignments = torch.zeros(max_length, encoder_outputs.size(1), batch_size) # attention align을 저장하기 위한 용도 \n","        logits = torch.zeros(max_length, batch_size, self.decoder.output_size) # logits 값을 저장하기 위한 용도의 tensor \n","\n","        if self.gpu:\n","            decoder_input = decoder_input.cuda()\n","            decoder_context = decoder_context.cuda()\n","            logits = logits.cuda()\n","        inference = []\n","        for t in range(max_length):\n","\n","            # The decoder accepts, at each time step t :\n","            # - an input, [B]\n","            # - a context, [B, H]\n","            # - an hidden state, [B, H]\n","            # - encoder outputs, [B, T, H]\n","            \n","            # The decoder outputs, at each time step t :\n","            # - an output, [B]\n","            # - a context, [B, H]\n","            # - an hidden state, [B, H]\n","            # - weights, [B, T]\n","\n","            outputs, decoder_hidden, attention_weights = self.decoder.forward(\n","                    input=decoder_input.long(),\n","                    prev_hidden=decoder_hidden,\n","                    encoder_outputs=encoder_outputs,\n","                    seq_len=input_lengths)\n","            \n","            alignments[t] = attention_weights.transpose(1, 0)\n","            \n","            \n","            logits[t] = outputs\n","\n","            \n","\n","            if  self.training:\n","                decoder_input = targets[:, t]\n","            else:\n","                topv, topi = outputs.data.topk(1) # 가장 높은 예측만 사용.\n","                decoder_input = topi.squeeze(-1).detach()\n","                inference.append(decoder_input.cpu())\n","\n","        \n","        labels = targets.contiguous().view(-1)\n","\n","        \n","        mask_value = 0\n","        #what is this mask_3d?\n","        logits = mask_3d(logits.transpose(1, 0), targets_lengths, mask_value)\n","        logits = logits.contiguous().view(-1, self.vocab_size) # loss를 구하기 위해 쫙 펴주기 \n","\n","        return logits, labels.long(), alignments,inference\n","\n","    \n","    def step(self, batch):\n","        x, y, x_len, y_len = batch\n","        if self.gpu:\n","            x = x.cuda()\n","            y = y.cuda()\n","        # x_len = x_len.cuda()\n","        # y_len = y_len.cuda()\n","\n","        encoder_out, encoder_state = self.encode(x, x_len) # encoder \n","        logits, labels, alignments,inference = self.decode(encoder_out, encoder_state, y, y_len, x_len) # decoder 를 통해 alignment와 logit 값 얻기 \n","        return logits, labels, alignments,inference\n","\n","    def loss(self, batch):\n","        logits, labels, alignments,inference = self.step(batch)\n","        loss = self.loss_fn(logits, labels) # loss 구하기 우리는 cross entropy 사용 \n","        return loss, logits, labels, alignments,inference"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xbOfjOmU6262"},"source":["### Train the model"]},{"cell_type":"markdown","metadata":{"id":"weB4hLTwfV3S"},"source":["![대체 텍스트](https://pytorch.org/tutorials/_images/seq2seq.png)"]},{"cell_type":"code","metadata":{"id":"NKYVKohKBmpS"},"source":["def train(model, optimizer, train_loader, epoch,n_epochs):\n","    \n","\n","    losses = []\n","    cers = []\n","\n","    \n","    model.train() # train mode \n","    count = 0\n","    for batch in train_loader:\n","        loss, _, _, _,_ = model.loss(batch)\n","        losses.append(loss.item())\n","        # Reset gradients\n","        optimizer.zero_grad()\n","        # Compute gradients\n","        loss.backward()\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2)\n","        optimizer.step()\n","  \n","    print ('\\n [{}/{}] avg_loss= {:.3f}'.format(epoch,n_epochs,np.mean(losses)))\n","    \n","    return model, optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Edit distance (편집거리 알고리즘) \n","\n","![대체 텍스트](https://raw.githubusercontent.com/sumitc91/data/master/askgif-blog/9e07d056-ccf7-4fc8-b6ee-000c8032b9ec_editDistance.gif)"],"metadata":{"id":"krIKDlLXjNOo"}},{"cell_type":"code","source":["# edit distance 란 편집 거리 \n","ref = [1, 2, 3, 4]\n","hyp = [1, 2, 4, 5, 6]\n","editdistance.eval(ref,hyp)"],"metadata":{"id":"107_ctN0jB3O"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0q5dl-qGtXB"},"source":["def evaluate(model, eval_loader):\n","\n","    losses = []\n","    accs = []\n","    edits = []\n","    \n","    model.eval() # why?? \n","\n","    with torch.no_grad():\n","        for batch in eval_loader:\n","            #t.set_description(\" Evaluating... (train={})\".format(model.training))\n","            loss, logits, labels, alignments,_ = model.loss(batch)\n","            preds = logits.detach().cpu().numpy()\n","            \n","            acc = 100 *np.sum(np.argmax(preds, -1) == labels.detach().cpu().numpy()) / len(preds)\n","            edit = editdistance.eval(np.argmax(preds, -1), labels.detach().cpu().numpy()) / len(preds)\n","            \n","            losses.append(loss.item())\n","            \n","            accs.append(acc)\n","            edits.append(edit)\n","        \n","        align = alignments.detach().cpu().numpy()[:, :, 0]\n","\n","   \n","    print(\"  End of evaluation : loss {:.3f} , acc {:.1f} , edits {:.3f}\".format(np.mean(losses), np.mean(accs), np.mean(edits)))\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWWvvbf68__6"},"source":["## 학습을 진행해보도록 하겠습니다"]},{"cell_type":"code","metadata":{"id":"N6zoFKQj8_MU"},"source":["USE_CUDA = torch.cuda.is_available()\n","batch_size = 32\n","epochs = 6\n","dataset = ToyDataset(5, 15)\n","eval_dataset = ToyDataset(5, 15, type='eval')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PKechkHv9EXQ"},"source":["train_loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate, drop_last=True)\n","eval_loader = data.DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate,drop_last=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JapBBgq6KNp2"},"source":[""]},{"cell_type":"code","metadata":{"id":"xh2HQkg59sND","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"error","timestamp":1660647881796,"user_tz":-540,"elapsed":326,"user":{"displayName":"김정의","userId":"05657577236532280758"}},"outputId":"94181783-e8f0-4f94-86e0-e8b20c59a145"},"source":["config = {\n","  \"decoder\": \"Bahdanau\",\n","  \"encoder\": \"RNN\",\n","  \"n_channels\": 4,\n","  \"encoder_hidden\": 64,\n","  \"encoder_layers\": 1,\n","  \"encoder_dropout\": 0.2,\n","  \"bidirectional_encoder\": True,\n","  \"decoder_hidden\": 64,\n","  \"decoder_layers\": 1,\n","  \"decoder_dropout\": 0.2,\n","  \"n_classes\":dataset.VOCAB_SIZE+3 ,\n","  \"batch_size\": 32,\n","  \"embedding_dim\": 64,\n","  \"attention_score\": \"bahdanau\",\n","  \"learning_rate\": 0.001,\n","  \"gpu\": True,\n","  \"loss\": \"cross_entropy\"\n","}"],"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-3c07eb63bebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;34m\"decoder_layers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;34m\"decoder_dropout\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;34m\"n_classes\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;34m\"embedding_dim\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"]}]},{"cell_type":"code","metadata":{"id":"N_h_LOmU-AEz"},"source":["model = Seq2Seq(config)\n","model = model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jXTGn9V-CuD"},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", 0.001))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ktMBioF-EAN"},"source":["for epoch in range(epochs):\n","  model,optimizer  = train(model, optimizer, train_loader, epoch, epochs)\n","  evaluate(model,eval_loader)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RNRT6acajT5"},"source":["### 시각화"]},{"cell_type":"code","metadata":{"id":"cQ77ZlPKal2j"},"source":["import seaborn\n","\n","def draw(data, x, y):\n","    seaborn.heatmap(data, \n","                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, \n","                    cbar=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h3fVzxjganHg"},"source":["def visualize_plot(model,custom_input= 'cgdafa'):\n","    c_xs = np.array([dataset.char2int.get(x) for x in custom_input]+[2])\n","    c_xs = torch.from_numpy(c_xs).unsqueeze(0).long()\n","\n","    c_xl = torch.tensor(c_xs[0].size()[-1]).unsqueeze(0)\n","\n","    c_ys = np.array([dataset.char2int.get(x) for x in custom_input[::-1]] + [2]) # Return the random string and its reverse + EOS \n","    c_ys = torch.from_numpy(c_ys).unsqueeze(0).long()\n","\n","    c_yl = torch.tensor(c_ys[0].size()[-1]).unsqueeze(0)\n","    c_data = (c_xs,c_ys,c_xl,c_yl)\n","    loss, logits, labels, alignments,predict=model.loss(c_data)\n","    heat_map_value = alignments.detach().cpu().numpy()[:, :, 0]\n","    preds = logits.detach().cpu().numpy()\n","    preds = np.argmax(preds, -1)\n","    source_tokens = [ dataset.int2char[item-3] for item in c_xs[0] if item!=0 if item !=2 ] +['</s>']\n","    target_tokens = [ dataset.int2char[item-3] if item !=2 else '</s>' for item in preds.tolist() if item!=0 ]\n","    draw(heat_map_value,source_tokens,target_tokens)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IkbDmwaY4BG"},"source":["visualize_plot(model,'cbada')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZZdAsEpjcC0V"},"source":["### Dot Mode "]},{"cell_type":"code","metadata":{"id":"T8X3wWrrewkY"},"source":["config = {\n","  \"decoder\": \"Bahdanau\",\n","  \"encoder\": \"RNN\",\n","  \"n_channels\": 4,\n","  \"encoder_hidden\": 64,\n","  \"encoder_layers\": 1,\n","  \"encoder_dropout\": 0.2,\n","  \"bidirectional_encoder\": False,\n","  \"decoder_hidden\": 64,\n","  \"decoder_layers\": 1,\n","  \"decoder_dropout\": 0.2,\n","  \"n_classes\":dataset.VOCAB_SIZE+3 ,\n","  \"batch_size\": 32,\n","  \"embedding_dim\": 64,\n","  \"attention_score\": \"concat\",\n","  \"learning_rate\": 0.001,\n","  \"gpu\": True,\n","  \"loss\": \"cross_entropy\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xHTy6m9k7XU"},"source":["model = Seq2Seq(config)\n","model = model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuAGn6wOcMAU"},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NNv1KexcOQh"},"source":["for epoch in range(epochs):\n","  model,optimizer  = train(model,optimizer, train_loader,epoch,epochs)\n","  evaluate(model,eval_loader)\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"48ACZ__zcgz3"},"source":["visualize_plot(model,'cbada')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZio7ZpScXWf"},"source":["### Concat Mode "]},{"cell_type":"code","metadata":{"id":"rlEMn4CDcVdv"},"source":["config = {\n","  \"decoder\": \"Bahdanau\",\n","  \"encoder\": \"RNN\",\n","  \"n_channels\": 4,\n","  \"encoder_hidden\": 64,\n","  \"encoder_layers\": 1,\n","  \"encoder_dropout\": 0.2,\n","  \"bidirectional_encoder\": False,\n","  \"decoder_hidden\": 64,\n","  \"decoder_layers\": 1,\n","  \"decoder_dropout\": 0.2,\n","  \"n_classes\":dataset.VOCAB_SIZE+3 ,\n","  \"batch_size\": 32,\n","  \"embedding_dim\": 64,\n","  \"attention_score\": \"concat\",\n","  \"learning_rate\": 0.001,\n","  \"gpu\": True,\n","  \"loss\": \"cross_entropy\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvMOke61cbvf"},"source":["model = Seq2Seq(config)\n","model = model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCRpTxKVcc__"},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjbdwe3jcfFD"},"source":["for epoch in range(epochs):\n","  model,optimizer  = train(model,optimizer, train_loader,epoch,epochs)\n","  evaluate(model,eval_loader)\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhnkBB82chid"},"source":["visualize_plot(model,'cbada')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMzGeSiUdZRo"},"source":[""],"execution_count":null,"outputs":[]}]}