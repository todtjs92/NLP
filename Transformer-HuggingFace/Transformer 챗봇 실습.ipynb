{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer á„á…¢á†ºá„‡á…©á†º á„‰á…µá†¯á„‰á…³á†¸.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### **í•„ìš” íŒ¨í‚¤ì§€ import**"],"metadata":{"id":"eoZwwSZxHOR1"}},{"cell_type":"code","source":["!pip install tokenizers # Huggingface - ğŸ¤— Tokenizers"],"metadata":{"id":"vl6qGf1GUVrR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import urllib.request\n","import time\n","import torch\n","import torch.nn as nn"],"metadata":{"id":"EW5ps47KOPPT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformerë¡œ ì±—ë´‡ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤!\n","- Transformerì— ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´, ê·¸ì— ëŒ€í•œ ëŒ€ë‹µì„ í•˜ëŠ” ì±—ë´‡ì„ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤.\n","- ì¤‘ê°„ì¤‘ê°„ ë¹ˆì¹¸ì„ ì±„ì›Œë„£ì–´ê°€ë©° ìì‹ ë§Œì˜ ì±—ë´‡ ì½”ë“œë¥¼ ì™„ì„±í•´ë³´ì„¸ìš”."],"metadata":{"id":"jOGAzogcSsmo"}},{"cell_type":"markdown","source":["ìš°ì„  ì±—ë´‡ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤."],"metadata":{"id":"Ew9i4J_ATGbW"}},{"cell_type":"code","source":["# data download\n","# from https://github.com/songys/Chatbot_data\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n","train_data = pd.read_csv('ChatBotData.csv')"],"metadata":{"id":"mHZkvZGmSuyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('ë°ì´í„° ìƒ˜í”Œì˜ ê°œìˆ˜ :', len(train_data))"],"metadata":{"id":"6ojYsR2aS9ag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.head(10)"],"metadata":{"id":"rh_q-eifTDiJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## í•œêµ­ì–´ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤.\n","- ìš°ë¦¬ëŠ” ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ë°ì´í„°ì— tokenizerë¥¼ í•™ìŠµì‹œì¼œ ë‹¨ì–´ë¥¼ êµ¬ë¶„í•˜ë„ë¡ í•  ì˜ˆì •ì…ë‹ˆë‹¤. \n","(ì±—ë´‡ ë°ì´í„°ì— ë¹„í•´ í›¨ì”¬ ë§ì€ ë°ì´í„°ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.)"],"metadata":{"id":"agVQBr16TZHR"}},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n","naver_df = pd.read_table('ratings.txt')\n","naver_df = naver_df.dropna(how='any')\n","with open('naver_review.txt', 'w', encoding='utf8') as f:\n","    f.write('\\n'.join(naver_df['document']))"],"metadata":{"id":"asKi2NoTTL0m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tokenizers import BertWordPieceTokenizer\n","tokenizer = BertWordPieceTokenizer(lowercase=False) # lowercaseë¥¼ êµ¬ë¶„í• ì§€ ì—¬ë¶€ë¥¼ ì„ íƒí•©ë‹ˆë‹¤ (True: ëŒ€ë¬¸ì ë¬´ì‹œ(ëª¨ë‘ ì†Œë¬¸ìë¡œ ì¸ì‹), False:ëŒ€ì†Œë¬¸ì êµ¬ë¶„)"],"metadata":{"id":"_Ksuv9T9T3KP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_file = 'naver_review.txt'\n","vocab_size = 30000\n","limit_alphabet = 6000\n","min_frequency = 5\n","\n","tokenizer.train(files=data_file,\n","                vocab_size=vocab_size,\n","                limit_alphabet=limit_alphabet,\n","                min_frequency=min_frequency,\n","                special_tokens=['[PAD]', '[START]', '[END]', '[UNK]'])"],"metadata":{"id":"CgwAEodSUPSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vocab ì €ì¥\n","tokenizer.save_model('./')\n","vocab_df = pd.read_fwf('vocab.txt', header=None)\n","vocab_df"],"metadata":{"id":"mmbXucF5Uu3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ì±—ë´‡ ëª¨ë¸ì€ í† í° ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ ì‘ë™ì‹œí‚¤ê³ , ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë„ ë§ˆì°¬ê°€ì§€ë¡œ í† í° ì¸ë±ìŠ¤ë¡œ ëœ ê²°ê³¼ë¥¼ ë‚´ë±‰ìŠµë‹ˆë‹¤.\n","# ì¸ì½”ë”©ëœ ê²°ê³¼ë¬¼ì„ decode í•¨ìˆ˜ë¥¼ í†µí•´ í•´ì„ ê°€ëŠ¥í•œ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¿”ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","encoded = tokenizer.encode('ì´ ì±—ë´‡ì´ ì˜ ì™„ì„±ë ê¹Œìš”?')\n","print('í† í°í™” ê²°ê³¼ :',encoded.tokens)\n","print('í† í° ì¸ë±ìŠ¤ :',encoded.ids)\n","print('ë””ì½”ë”© :',tokenizer.decode(encoded.ids))"],"metadata":{"id":"na8cPqVZVNB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer"],"metadata":{"id":"IxK16jsQXOzj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformer ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤."],"metadata":{"id":"_KxCM1IiVtbA"}},{"cell_type":"code","source":["from torch import Tensor\n","import torch\n","import torch.nn as nn\n","import math\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n","class PositionalEncoding(nn.Module):\n","    def __init__(self,\n","                 emb_size: int,\n","                 dropout: float,\n","                 maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding) # registerì— ë“±ë¡í•˜ë©´ íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡ì´ ì•ˆëœë‹¤ê³  í•œë‹¤ . , íŒŒë¼ë¯¸í„°ì— ì—…ë°ì´íŠ¸ëŠ” ì•ˆë˜ë©´ì„œ ëª¨ë¸ ìŠ¤í…Œì´íŠ¸ì—ëŠ” ì €ì¥ë¨. \n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size) # ì›Œë“œì„ë² ë”©ì´ , í¬ì§€ì…˜ë³´ë‹¤ ì¤‘ìš”í•´ ê·¸ë˜ì„œ ì›Œë“œì„ë² ë”©ì— ì˜í–¥ë”ì£¼ê¸°ìœ„í•´ ì´ëŸ°ì‹ìœ¼ë¡œ í•´ì¤¬ë‹¤ê³  í•¨. \n","\n","# Seq2Seq Network\n","class Seq2SeqTransformer(nn.Module): # ë””ì½”ë” ë’¤ì— ë¶€ë¶„ \n","    def __init__(self,\n","                 num_encoder_layers: int,\n","                 num_decoder_layers: int,\n","                 emb_size: int,\n","                 nhead: int,\n","                 src_vocab_size: int,\n","                 tgt_vocab_size: int,\n","                 dim_feedforward: int = 512,\n","                 dropout: float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        # ============== TODO ======================\n","        # torch.nn ì—ëŠ” Transformer ëª¨ë¸ì´ ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \n","        # ì´ê²ƒì„ ì´ìš©í•´ ì±—ë´‡ ëª¨ë¸ì„ ì™„ì„±í•´ë´…ì‹œë‹¤.\n","        # ==========================================\n","        self.transformer = nn.Transformer(d_model=emb_size, # ì›Œë“œì„ë² ë”©ì´ë‘ ê°™ì€ ì‚¬ì´ì¦ˆ ì”€ .       # nnì— íŠ¸ëœìŠ¤í¬ë¨¸ê°€ êµ¬í˜„ë˜ìˆìŒ. ì¸ì½”ë”©ì˜ ë¸”ë¡ ë¶€ë¶„ì´ë‘ , ë””ì½”ë”©ì˜ ë¸”ë¡ë¶€ë¶„ë§Œ êµ¬í˜„ë˜ìˆìŒ. \n","                                       nhead=nhead,      # í—¤ë“œì˜ ìˆ˜           # freeze= True ì¨ë„ ë¨ ê± . \n","                                       num_encoder_layers=num_encoder_layers, # ì¸ì½”ë” ë¸”ë¡ì˜ ê°œìˆ˜ .\n","                                       num_decoder_layers=num_decoder_layers,\n","                                       dim_feedforward=dim_feedforward, # í”¼ë“œ í¬ì›Œë“œì˜ ë””ë©˜ì…˜ \n","                                       dropout=dropout) \n","        self.generator = nn.Linear(emb_size, tgt_vocab_size) # ë³´ì¼€ë·¸ëŸ¬ë¦¬ ì‚¬ì´ì¦ˆë¡œ ì¤„ì„ . \n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size) # ì¸ì½”ë”ìª½ì— ì…ë ¥ë˜ëŠ” ì„ë² ë”© .\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size) # ë””ì½”ë”\n","        self.positional_encoding = PositionalEncoding(\n","            emb_size, dropout=dropout)\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                src_mask: Tensor,\n","                tgt_mask: Tensor,\n","                src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor,\n","                memory_key_padding_mask: Tensor):\n","        # ============== TODO ======================\n","        # ìœ„ì—ì„œ êµ¬í˜„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì— ì…ë ¥ëœ ë°ì´í„°ë¥¼ í†µê³¼ì‹œì¼œë´…ì‹œë‹¤.\n","        # ==========================================\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n","                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n","        return self.generator(outs)\n","\n","    def encode(self, src: Tensor, src_mask: Tensor):             # í”„ë¦¬ë”•ì…˜ ìš©ì„. \n","        return self.transformer.encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor): # í”„ë¦¬ë”•ì…˜ ìš© \n","        return self.transformer.decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)"],"metadata":{"id":"sML-QsR1VSiR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Transformerì˜ í•™ìŠµ ì¤‘ì—ëŠ”, ë¯¸ë˜ì˜ ë°ì´í„°ë¥¼ ë³¼ ìˆ˜ ì—†ë„ë¡ í•˜ëŠ” ë§ˆìŠ¤í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."],"metadata":{"id":"fGoPx7h4Wtxe"}},{"cell_type":"code","source":["PAD_IDX = 0\n","def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)) \n","    return mask \n","\n","\n","def create_mask(src, tgt):\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len) # If a FloatTensor is provided, it will be added to the attention weight.\n","    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool) #If a BoolTensor is provided, False values will be unchanged. \n","\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1) #If a BoolTensor is provided, positions with True are not allowed to attend.\n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"],"metadata":{"id":"se0BOkYxWptG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SRC_VOCAB_SIZE = vocab_size # 30000\n","TGT_VOCAB_SIZE = vocab_size # 30000\n","EMB_SIZE = 512\n","NHEAD = 8\n","FFN_HID_DIM = 512\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","\n","# ëª¨ë¸ì„ initialize í•´ì¤ë‹ˆë‹¤.\n","model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n","                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n","model = model.to(DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX) # ignore_index ê°€ í•˜ëŠ” ì—­í• ì€ ë¬´ì—‡ì¼ê¹Œìš”?\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"],"metadata":{"id":"Z1mFnKa_W46Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","class ChatbotDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        x, y, _ = self.data[idx]\n","        # ============== TODO ==============\n","        # 1. ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì¦ˆ í•´ë³´ì„¸ìš”. (x,y ëª¨ë‘)\n","        # 2. start token ê³¼ end tokenì„ ì¶”ê°€í•´ë³´ì„¸ìš”. (x,y ëª¨ë‘)\n","        # 3. í† í°ë“¤ì„ torch tensor (long íƒ€ì…)ë¡œ ë³€í™˜í•˜ê³  returní•˜ì„¸ìš”.\n","        # ==================================\n","\n","        x_tokens = self.tokenizer.encode(x).ids\n","        y_tokens = self.tokenizer.encode(y).ids\n","\n","        x_tokens.insert(0,1)\n","        x_tokens.append(2)\n","        y_tokens.insert(0,1)\n","        y_tokens.append(2)\n","\n","        return torch.tensor(x_tokens).long(), torch.tensor(y_tokens).long()"],"metadata":{"id":"yrdvopXGXene"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = ChatbotDataset(train_data.values, tokenizer)"],"metadata":{"id":"zV77ccjfYaSO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]"],"metadata":{"id":"S-2OOeplYe1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"5M6IqKpLAV3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# collate_fn ì„ ì •ì˜í•´ì¤ë‹ˆë‹¤.\n","# datasetìœ¼ë¡œë¶€í„° ì—¬ëŸ¬ itemì„ ë°›ì•„ì™€ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ í•©ì¹  ë•Œ, ì–´ë–»ê²Œ í•©ì¹ ì§€ë¥¼ ì •ì˜í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.\n","\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","def collate_fn(batch):\n","    # batch: [(x1, y1), (x2, y2), ... (x4, y4)]\n","    # ============== TODO ==============\n","    # ìš°ë¦¬ì˜ ëª©í‘œëŠ” X = [x1,x2,x3,...] , Y = [y1, y2, y3, ...] í˜•íƒœì˜ tensorë¡œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.\n","    # ì´ë¥¼ ìœ„í•´ 2ê°€ì§€ í•´ì•¼í•  ì¼ì´ ìˆìŠµë‹ˆë‹¤.\n","    # 1. batch ì•ˆì— ìˆëŠ” x ì™€ yë¥¼ ê°ê°ì˜ list ì— ëª¨ì•„ì£¼ê¸°.\n","    # 2. paddingì„ í†µí•´ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§Œë“¤ì–´ì£¼ê³ , í•˜ë‚˜ì˜ tensorë¡œ í†µí•©í•˜ê¸°.\n","    # ì•„ë˜ ë¹ˆì¹¸ì„ ì±„ì›Œ ìœ„ ë‘ê°€ì§€ë¥¼ ì§„í–‰í•´ë³´ì„¸ìš”.\n","    # ==================================\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(src_sample)\n","        tgt_batch.append(tgt_sample)\n","    \n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    return src_batch, tgt_batch\n","\n","# dataloader ë¥¼ ì •ì˜í•´ì¤ë‹ˆë‹¤.\n","batch_size = 32\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n","                         num_workers=2, collate_fn=collate_fn,\n","                          pin_memory=True, drop_last=True)"],"metadata":{"id":"Ghm16BskYotq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x,y = next(iter(data_loader))"],"metadata":{"id":"djH7TslZZYnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape, y.shape"],"metadata":{"id":"D6og4pAHZrTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x[:,2]"],"metadata":{"id":"LBJbclAJZv1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","def train_epoch(model, optimizer, loss_fn, train_dataloader):\n","    model.train()\n","    losses = 0\n","\n","    for src, tgt in train_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        # (seq_length, batch_size)\n","        tgt_input = tgt[:-1,:] # ì™œ targetì€ ë§ˆì§€ë§‰ í•˜ë‚˜ë¥¼ ë¹¼ê³  ì…ë ¥í• ê¹Œìš”? # <sos > i love you //// <eos>ëŠ” íŒ¨ë”©ì´ë¼ ëº´ì¤Œ . \n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        optimizer.zero_grad()\n","\n","        # tgt_out (seq_length, batch_size)\n","        tgt_out = tgt[1:, :] # ì™œ ì²«ë²ˆì§¸ ë‹¨ì–´ëŠ” ë¹¼ê³  ë¡œìŠ¤ë¥¼ ê³„ì‚°í• ê¹Œìš”? # ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•´ <eos> ìš”ë ‡ê²Œ \n","\n","\n","        # cross entropy: x (, class), y (*,)\n","        # logits (seq_length * batch_size, vocab_size)\n","        # tgt_out (seq_length * batch_size)\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(train_dataloader)"],"metadata":{"id":"vT3K6ov3Z5PX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 20\n","for i in range(EPOCHS):\n","    epoch_loss = train_epoch(model, optimizer, loss_fn, data_loader)\n","    print('EPOCH {} LOSS {:.6f}'.format(i+1, epoch_loss))"],"metadata":{"id":"8ZQY8en1cRtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def greedy_decode(model, src, src_mask, max_len, start_symbol):  # beamì„œì¹˜ ë¶€ë¶„ì´ë˜\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask) # ì¸ì½”ë” ê²°ê³¼ë¶€ë¶„\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE) # ys = ë””ì½”ë” ì¸í’‹\n","    # ys = [[1]] sosë¥¼ ë„£ëŠ”ê±°ì„ .  \n","    # ìµœëŒ€ê¸¸ì´ê°€ ë ë•Œê¹Œì§€ ë°˜ë³µí•˜ê²Œ ë˜ëŠ”ê±° . \n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask) # \n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.item()\n","\n","        # ì•„ë˜ì— ys ì»¨ìº£í•´ì£¼ëŠ” ë¶€ë¶„ì´ ìˆìŒ. ì»¨ìº£í•˜ë©´  (2 ë¬¸ì¥ê¸¸ì´,1= ë°°ì¹˜) -> (3 ë¬¸ì¥ã„·ê¸¸ì´ , 1= ë°°ì¹˜ )\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == 2:\n","            break\n","    return ys\n","\n","def predict(model, tokenizer, src_sentence):\n","    with torch.no_grad():\n","        model.eval()\n","        src = tokenizer.encode(src_sentence).ids\n","        src.insert(0,1)\n","        src.append(2)\n","        src = torch.tensor(src).long().unsqueeze(1)\n","        num_tokens = src.shape[0]\n","        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","        tgt_tokens = greedy_decode(\n","            model,  src, src_mask, max_len=num_tokens + 5, start_symbol=1).flatten()\n","    return tokenizer.decode(tgt_tokens.cpu().tolist())"],"metadata":{"id":"8SgrMYEtcnmU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(model,tokenizer,'ì˜ ì§€ë‚´?')"],"metadata":{"id":"fHidx9_zh2A4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(model,tokenizer,'ì—¬í–‰ ê°€ê³  ì‹¶ë‹¤.')"],"metadata":{"id":"NozJbX0Ohzzp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","  question = input('user1 : ')\n","  if question.lower == 'bye bye':\n","    break\n","  else:\n","    answer = predict(model,tokenizer, question)\n","    print('Bot : {}'.format(answer))"],"metadata":{"id":"aXxlj7B72bwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"d9iFyaa78aKB"},"execution_count":null,"outputs":[]}]}