{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer 챗봇 실습.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### **필요 패키지 import**"],"metadata":{"id":"eoZwwSZxHOR1"}},{"cell_type":"code","source":["!pip install tokenizers # Huggingface - 🤗 Tokenizers"],"metadata":{"id":"vl6qGf1GUVrR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import urllib.request\n","import time\n","import torch\n","import torch.nn as nn"],"metadata":{"id":"EW5ps47KOPPT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformer로 챗봇을 만들어봅시다!\n","- Transformer에 문장을 입력하면, 그에 대한 대답을 하는 챗봇을 만들 것입니다.\n","- 중간중간 빈칸을 채워넣어가며 자신만의 챗봇 코드를 완성해보세요."],"metadata":{"id":"jOGAzogcSsmo"}},{"cell_type":"markdown","source":["우선 챗봇 학습을 위한 데이터를 다운로드 받습니다."],"metadata":{"id":"Ew9i4J_ATGbW"}},{"cell_type":"code","source":["# data download\n","# from https://github.com/songys/Chatbot_data\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n","train_data = pd.read_csv('ChatBotData.csv')"],"metadata":{"id":"mHZkvZGmSuyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('데이터 샘플의 개수 :', len(train_data))"],"metadata":{"id":"6ojYsR2aS9ag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.head(10)"],"metadata":{"id":"rh_q-eifTDiJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 한국어 데이터를 전처리하는 방법에 대해 알아봅시다.\n","- 우리는 네이버 영화리뷰 데이터에 tokenizer를 학습시켜 단어를 구분하도록 할 예정입니다. \n","(챗봇 데이터에 비해 훨씬 많은 데이터를 포함하고 있습니다.)"],"metadata":{"id":"agVQBr16TZHR"}},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n","naver_df = pd.read_table('ratings.txt')\n","naver_df = naver_df.dropna(how='any')\n","with open('naver_review.txt', 'w', encoding='utf8') as f:\n","    f.write('\\n'.join(naver_df['document']))"],"metadata":{"id":"asKi2NoTTL0m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tokenizers import BertWordPieceTokenizer\n","tokenizer = BertWordPieceTokenizer(lowercase=False) # lowercase를 구분할지 여부를 선택합니다 (True: 대문자 무시(모두 소문자로 인식), False:대소문자 구분)"],"metadata":{"id":"_Ksuv9T9T3KP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_file = 'naver_review.txt'\n","vocab_size = 30000\n","limit_alphabet = 6000\n","min_frequency = 5\n","\n","tokenizer.train(files=data_file,\n","                vocab_size=vocab_size,\n","                limit_alphabet=limit_alphabet,\n","                min_frequency=min_frequency,\n","                special_tokens=['[PAD]', '[START]', '[END]', '[UNK]'])"],"metadata":{"id":"CgwAEodSUPSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vocab 저장\n","tokenizer.save_model('./')\n","vocab_df = pd.read_fwf('vocab.txt', header=None)\n","vocab_df"],"metadata":{"id":"mmbXucF5Uu3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 챗봇 모델은 토큰 인덱스를 이용하여 모델을 작동시키고, 최종 예측 결과도 마찬가지로 토큰 인덱스로 된 결과를 내뱉습니다.\n","# 인코딩된 결과물을 decode 함수를 통해 해석 가능한 문장으로 바꿔줄 수 있습니다.\n","encoded = tokenizer.encode('이 챗봇이 잘 완성될까요?')\n","print('토큰화 결과 :',encoded.tokens)\n","print('토큰 인덱스 :',encoded.ids)\n","print('디코딩 :',tokenizer.decode(encoded.ids))"],"metadata":{"id":"na8cPqVZVNB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer"],"metadata":{"id":"IxK16jsQXOzj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformer 모델을 만들어봅시다."],"metadata":{"id":"_KxCM1IiVtbA"}},{"cell_type":"code","source":["from torch import Tensor\n","import torch\n","import torch.nn as nn\n","import math\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n","class PositionalEncoding(nn.Module):\n","    def __init__(self,\n","                 emb_size: int,\n","                 dropout: float,\n","                 maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding) # register에 등록하면 파라미터로 등록이 안된다고 한다 . , 파라미터에 업데이트는 안되면서 모델 스테이트에는 저장됨. \n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size) # 워드임베딩이 , 포지션보다 중요해 그래서 워드임베딩에 영향더주기위해 이런식으로 해줬다고 함. \n","\n","# Seq2Seq Network\n","class Seq2SeqTransformer(nn.Module): # 디코더 뒤에 부분 \n","    def __init__(self,\n","                 num_encoder_layers: int,\n","                 num_decoder_layers: int,\n","                 emb_size: int,\n","                 nhead: int,\n","                 src_vocab_size: int,\n","                 tgt_vocab_size: int,\n","                 dim_feedforward: int = 512,\n","                 dropout: float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        # ============== TODO ======================\n","        # torch.nn 에는 Transformer 모델이 이미 구현되어 있습니다. \n","        # 이것을 이용해 챗봇 모델을 완성해봅시다.\n","        # ==========================================\n","        self.transformer = nn.Transformer(d_model=emb_size, # 워드임베딩이랑 같은 사이즈 씀 .       # nn에 트랜스포머가 구현되있음. 인코딩의 블록 부분이랑 , 디코딩의 블록부분만 구현되있음. \n","                                       nhead=nhead,      # 헤드의 수           # freeze= True 써도 됨 걍 . \n","                                       num_encoder_layers=num_encoder_layers, # 인코더 블록의 개수 .\n","                                       num_decoder_layers=num_decoder_layers,\n","                                       dim_feedforward=dim_feedforward, # 피드 포워드의 디멘션 \n","                                       dropout=dropout) \n","        self.generator = nn.Linear(emb_size, tgt_vocab_size) # 보케뷸러리 사이즈로 줄임 . \n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size) # 인코더쪽에 입력되는 임베딩 .\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size) # 디코더\n","        self.positional_encoding = PositionalEncoding(\n","            emb_size, dropout=dropout)\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                src_mask: Tensor,\n","                tgt_mask: Tensor,\n","                src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor,\n","                memory_key_padding_mask: Tensor):\n","        # ============== TODO ======================\n","        # 위에서 구현한 트랜스포머 모델에 입력된 데이터를 통과시켜봅시다.\n","        # ==========================================\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n","                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n","        return self.generator(outs)\n","\n","    def encode(self, src: Tensor, src_mask: Tensor):             # 프리딕션 용임. \n","        return self.transformer.encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor): # 프리딕션 용 \n","        return self.transformer.decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)"],"metadata":{"id":"sML-QsR1VSiR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Transformer의 학습 중에는, 미래의 데이터를 볼 수 없도록 하는 마스크가 필요합니다."],"metadata":{"id":"fGoPx7h4Wtxe"}},{"cell_type":"code","source":["PAD_IDX = 0\n","def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)) \n","    return mask \n","\n","\n","def create_mask(src, tgt):\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len) # If a FloatTensor is provided, it will be added to the attention weight.\n","    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool) #If a BoolTensor is provided, False values will be unchanged. \n","\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1) #If a BoolTensor is provided, positions with True are not allowed to attend.\n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"],"metadata":{"id":"se0BOkYxWptG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SRC_VOCAB_SIZE = vocab_size # 30000\n","TGT_VOCAB_SIZE = vocab_size # 30000\n","EMB_SIZE = 512\n","NHEAD = 8\n","FFN_HID_DIM = 512\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","\n","# 모델을 initialize 해줍니다.\n","model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n","                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n","model = model.to(DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX) # ignore_index 가 하는 역할은 무엇일까요?\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"],"metadata":{"id":"Z1mFnKa_W46Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","class ChatbotDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        x, y, _ = self.data[idx]\n","        # ============== TODO ==============\n","        # 1. 데이터를 토크나이즈 해보세요. (x,y 모두)\n","        # 2. start token 과 end token을 추가해보세요. (x,y 모두)\n","        # 3. 토큰들을 torch tensor (long 타입)로 변환하고 return하세요.\n","        # ==================================\n","\n","        x_tokens = self.tokenizer.encode(x).ids\n","        y_tokens = self.tokenizer.encode(y).ids\n","\n","        x_tokens.insert(0,1)\n","        x_tokens.append(2)\n","        y_tokens.insert(0,1)\n","        y_tokens.append(2)\n","\n","        return torch.tensor(x_tokens).long(), torch.tensor(y_tokens).long()"],"metadata":{"id":"yrdvopXGXene"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = ChatbotDataset(train_data.values, tokenizer)"],"metadata":{"id":"zV77ccjfYaSO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]"],"metadata":{"id":"S-2OOeplYe1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"5M6IqKpLAV3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# collate_fn 을 정의해줍니다.\n","# dataset으로부터 여러 item을 받아와 하나의 배치로 합칠 때, 어떻게 합칠지를 정의하는 부분입니다.\n","\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","def collate_fn(batch):\n","    # batch: [(x1, y1), (x2, y2), ... (x4, y4)]\n","    # ============== TODO ==============\n","    # 우리의 목표는 X = [x1,x2,x3,...] , Y = [y1, y2, y3, ...] 형태의 tensor로 만드는 것입니다.\n","    # 이를 위해 2가지 해야할 일이 있습니다.\n","    # 1. batch 안에 있는 x 와 y를 각각의 list 에 모아주기.\n","    # 2. padding을 통해 동일한 길이로 만들어주고, 하나의 tensor로 통합하기.\n","    # 아래 빈칸을 채워 위 두가지를 진행해보세요.\n","    # ==================================\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(src_sample)\n","        tgt_batch.append(tgt_sample)\n","    \n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    return src_batch, tgt_batch\n","\n","# dataloader 를 정의해줍니다.\n","batch_size = 32\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n","                         num_workers=2, collate_fn=collate_fn,\n","                          pin_memory=True, drop_last=True)"],"metadata":{"id":"Ghm16BskYotq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x,y = next(iter(data_loader))"],"metadata":{"id":"djH7TslZZYnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape, y.shape"],"metadata":{"id":"D6og4pAHZrTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x[:,2]"],"metadata":{"id":"LBJbclAJZv1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","def train_epoch(model, optimizer, loss_fn, train_dataloader):\n","    model.train()\n","    losses = 0\n","\n","    for src, tgt in train_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        # (seq_length, batch_size)\n","        tgt_input = tgt[:-1,:] # 왜 target은 마지막 하나를 빼고 입력할까요? # <sos > i love you //// <eos>는 패딩이라 뺴줌 . \n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        optimizer.zero_grad()\n","\n","        # tgt_out (seq_length, batch_size)\n","        tgt_out = tgt[1:, :] # 왜 첫번째 단어는 빼고 로스를 계산할까요? # 나는 너를 사랑해 <eos> 요렇게 \n","\n","\n","        # cross entropy: x (, class), y (*,)\n","        # logits (seq_length * batch_size, vocab_size)\n","        # tgt_out (seq_length * batch_size)\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(train_dataloader)"],"metadata":{"id":"vT3K6ov3Z5PX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 20\n","for i in range(EPOCHS):\n","    epoch_loss = train_epoch(model, optimizer, loss_fn, data_loader)\n","    print('EPOCH {} LOSS {:.6f}'.format(i+1, epoch_loss))"],"metadata":{"id":"8ZQY8en1cRtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def greedy_decode(model, src, src_mask, max_len, start_symbol):  # beam서치 부분이래\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask) # 인코더 결과부분\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE) # ys = 디코더 인풋\n","    # ys = [[1]] sos를 넣는거임 .  \n","    # 최대길이가 될때까지 반복하게 되는거 . \n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask) # \n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.item()\n","\n","        # 아래에 ys 컨캣해주는 부분이 있음. 컨캣하면  (2 문장길이,1= 배치) -> (3 문장ㄷ길이 , 1= 배치 )\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == 2:\n","            break\n","    return ys\n","\n","def predict(model, tokenizer, src_sentence):\n","    with torch.no_grad():\n","        model.eval()\n","        src = tokenizer.encode(src_sentence).ids\n","        src.insert(0,1)\n","        src.append(2)\n","        src = torch.tensor(src).long().unsqueeze(1)\n","        num_tokens = src.shape[0]\n","        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","        tgt_tokens = greedy_decode(\n","            model,  src, src_mask, max_len=num_tokens + 5, start_symbol=1).flatten()\n","    return tokenizer.decode(tgt_tokens.cpu().tolist())"],"metadata":{"id":"8SgrMYEtcnmU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(model,tokenizer,'잘 지내?')"],"metadata":{"id":"fHidx9_zh2A4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(model,tokenizer,'여행 가고 싶다.')"],"metadata":{"id":"NozJbX0Ohzzp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","  question = input('user1 : ')\n","  if question.lower == 'bye bye':\n","    break\n","  else:\n","    answer = predict(model,tokenizer, question)\n","    print('Bot : {}'.format(answer))"],"metadata":{"id":"aXxlj7B72bwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"d9iFyaa78aKB"},"execution_count":null,"outputs":[]}]}